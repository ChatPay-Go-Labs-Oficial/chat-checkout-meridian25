import streamlit as st
import os
from dotenv import load_dotenv
import chromadb
from openai import OpenAI
from chromadb.utils import embedding_functions
import time
from mensagem_boas_vindas import get_mensagem_boas_vindas

# Carrega as vari√°veis de ambiente
load_dotenv()

class ChatRAGWeb:
    def __init__(self):
        """Inicializa o sistema de chat RAG para web"""
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        
        # Configura√ß√£o do embedding
        self.openai_ef = embedding_functions.OpenAIEmbeddingFunction(
            api_key=self.openai_api_key,
            model_name="text-embedding-3-small"
        )
        
        # Inicializa o cliente Chroma
        self.chroma_client = chromadb.PersistentClient(path="chroma_persistent_storage")
        self.collection_name = "texto_gerado"
        self.collection = self.chroma_client.get_or_create_collection(
            name=self.collection_name, embedding_function=self.openai_ef
        )
        
        # Configura√ß√£o da pasta de documentos
        self.docs_path = "docs"  # Pasta onde est√£o os arquivos .txt para treinamento
        
        # Cliente OpenAI
        self.client = OpenAI(api_key=self.openai_api_key)
        
        # Verifica se a pasta docs existe
        self.verificar_pasta_docs()

    def get_mensagem_boas_vindas(self):
        """Retorna a mensagem de boas-vindas configurada"""
        return get_mensagem_boas_vindas()

    def verificar_pasta_docs(self):
        """Verifica se a pasta docs existe e mostra informa√ß√µes"""
        if os.path.exists(self.docs_path):
            arquivos = [f for f in os.listdir(self.docs_path) if f.endswith('.txt')]
            print(f"‚úÖ Pasta {self.docs_path}/ encontrada com {len(arquivos)} arquivos .txt")
            for arquivo in arquivos:
                print(f"   üìÑ {arquivo}")
        else:
            print(f"‚ùå Pasta {self.docs_path}/ n√£o encontrada!")
            print("üîß Execute 'python rag.py' primeiro para processar os documentos")

    def query_documents(self, question, n_results=3):
        """Busca documentos relevantes na base de conhecimento"""
        try:
            results = self.collection.query(query_texts=question, n_results=n_results)
            relevant_chunks = [doc for sublist in results["documents"] for doc in sublist]
            return relevant_chunks
        except Exception as e:
            st.error(f"Erro ao buscar documentos: {e}")
            return []

    def generate_response(self, question, relevant_chunks, conversation_history):
        """Gera resposta usando OpenAI com contexto RAG"""
        try:
            context = "\n\n".join(relevant_chunks)
            
            # Prompt otimizado para vendas
            system_prompt = (
                "Voc√™ √© um assistente de vendas especializado em resolver obje√ß√µes e vender. "
                "Seu principal objetivo √© vender o produto 'Menos Caf√© Mais Ch√°'"
                "Use as informa√ß√µes do contexto para responder de forma emp√°tica, persuasiva e profissional. "
                "Use um tom conversacional, acolhedor e use emojis quando apropriado. "
                "Se n√£o souber a resposta baseada no contexto, diga que n√£o tem essa informa√ß√£o espec√≠fica. "
                "N√£o diga tudo o que √© poss√≠vel encontrar no produto, d√™ alguns detalhes mas n√£o todos"
                "Sempre fa√ßa uma chamanda para a venda, o objetivo √© fazer o cliente comprar o produto e n√£o simplesmente responder perguntas"
                "N√£o fale de desconto"
                "Sempre foque nos benef√≠cios e na solu√ß√£o que o produto oferece.\n\n"
                f"Contexto:\n{context}\n\n"
            )
            
            # Adiciona hist√≥rico da conversa para contexto
            messages = [{"role": "system", "content": system_prompt}]
            
            # Adiciona √∫ltimas 6 mensagens do hist√≥rico (3 perguntas + 3 respostas)
            for msg in conversation_history[-6:]:
                messages.append(msg)
            
            # Adiciona a pergunta atual
            messages.append({"role": "user", "content": question})
            
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                temperature=0.7,
                max_tokens=500
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            return f"‚ùå Erro ao gerar resposta: {e}"

def main():
    """Fun√ß√£o principal do Streamlit"""
    st.set_page_config(
        page_title="Chat de Vendas - Menos Caf√© Mais Ch√°",
        page_icon="‚òï",
        layout="wide"
    )
    
    # T√≠tulo e descri√ß√£o
    st.title("‚òï Chat de Vendas - Menos Caf√© Mais Ch√°")
    st.markdown("---")
    
    # Sidebar com informa√ß√µes
    with st.sidebar:
        st.header("üìã Sobre o Chat")
        st.markdown("""
        Este chat √© especializado em:
        - üéØ Resolver obje√ß√µes de vendas
        - ‚òï Informa√ß√µes sobre o produto
        - üõí Ajuda no checkout
        - üí° Dicas de convers√£o
        """)
        
        st.header("üí° Exemplos de Perguntas")
        st.markdown("""
        **Obje√ß√µes:**
        - "E se n√£o funcionar comigo?"
        - "Est√° muito caro"
        - "N√£o tenho tempo"
        
        **Produto:**
        - "Como funciona o m√©todo?"
        - "Quais os benef√≠cios?"
        - "Como preparar o ch√°?"
        """)
        
        if st.button("üßπ Limpar Chat"):
            st.session_state.messages = []
            # Adiciona mensagem de boas-vindas novamente
            st.session_state.messages.append(st.session_state.chat_rag.get_mensagem_boas_vindas())
            st.rerun()
    
    # Inicializa o chat RAG
    if "chat_rag" not in st.session_state:
        with st.spinner("ü§ñ Inicializando sistema..."):
            st.session_state.chat_rag = ChatRAGWeb()
    
    # Inicializa o hist√≥rico de mensagens
    if "messages" not in st.session_state:
        st.session_state.messages = []
        
        # Adiciona a mensagem de boas-vindas ao hist√≥rico
        st.session_state.messages.append(st.session_state.chat_rag.get_mensagem_boas_vindas())
    
    # Exibe o hist√≥rico de mensagens
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Input do usu√°rio
    if prompt := st.chat_input("Digite sua pergunta sobre obje√ß√µes ou o produto..."):
        # Adiciona a mensagem do usu√°rio
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Gera resposta
        with st.chat_message("assistant"):
            with st.spinner("üîç Buscando informa√ß√µes..."):
                # Busca documentos relevantes
                relevant_chunks = st.session_state.chat_rag.query_documents(prompt)
                
                if not relevant_chunks:
                    response = "‚ùå N√£o encontrei informa√ß√µes relevantes para sua pergunta. Tente reformular ou perguntar sobre obje√ß√µes de vendas ou o produto 'Menos Caf√© Mais Ch√°'."
                else:
                    # Gera resposta
                    response = st.session_state.chat_rag.generate_response(
                        prompt, relevant_chunks, st.session_state.messages
                    )
            
            st.markdown(response)
        
        # Adiciona a resposta ao hist√≥rico
        st.session_state.messages.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    main()
